## Smoke tests for tokenizers included in the analysis-common module

"keyword":
    - do:
        indices.analyze:
          body:
            text:      Foo Bar!
            tokenizer: keyword
    - length:    { tokens: 1 }
    - match:     { tokens.0.token: Foo Bar! }

---
"nGram":
    - do:
        indices.analyze:
          body:
            text: good
            explain: true
            tokenizer:
              type: nGram
              min_gram: 2
              max_gram: 2
    - length: { detail.tokenizer.tokens: 3 }
    - match:  { detail.tokenizer.name: _anonymous_tokenizer }
    - match:  { detail.tokenizer.tokens.0.token: go }
    - match:  { detail.tokenizer.tokens.1.token: oo }
    - match:  { detail.tokenizer.tokens.2.token: od }

---
"nGram_exception":
    - skip:
        version: " - 6.0.99"
        reason: starting from version 6.1.0 this produces a warning
        features: "warnings"
    - do:
        indices.analyze:
          body:
            text: good
            explain: true
            tokenizer:
              type: nGram
              min_gram: 2
              max_gram: 4
        warnings:
          - "Deprecated big difference between max_gram and min_gram in NGram Tokenizer,expected difference must be less than or equal to: [1]"
---
"simple_pattern":
    - do:
        indices.analyze:
          body:
            text: "a6bf fooo ff61"
            explain: true
            tokenizer:
              type: simple_pattern
              pattern: "[abcdef0123456789]{4}"
    - length: { detail.tokenizer.tokens: 2 }
    - match:  { detail.tokenizer.name: _anonymous_tokenizer }
    - match:  { detail.tokenizer.tokens.0.token: a6bf }
    - match:  { detail.tokenizer.tokens.1.token: ff61 }

---
"simple_pattern_split":
    - do:
        indices.analyze:
          body:
            text: "foo==bar"
            explain: true
            tokenizer:
              type: simple_pattern_split
              pattern: ==
    - length: { detail.tokenizer.tokens: 2 }
    - match:  { detail.tokenizer.name: _anonymous_tokenizer }
    - match:  { detail.tokenizer.tokens.0.token: foo }
    - match:  { detail.tokenizer.tokens.1.token: bar }
